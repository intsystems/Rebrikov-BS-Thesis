\section{Введение}

В последние годы в области машинного обучения наблюдается значительный прогресс, обусловленный стремлением к повышению качества решений и возможности решать всё более сложные задачи. Это привело к заметному увеличению как объёма данных, так и масштабов моделей. Данные изменения являются критически важными, поскольку способствуют стабилизации результатов и повышению точности выполнения задач.

Большинство задач машинного обучения сводится к задаче минимизации суммы конечного числа функций:
\[
\underset{x\in\mathbb R^d}{\min}\left[f(x) = \frac{1}{n}\sum\limits_{i=1}^n f_i(x)\right],
\]
где \(f_i: \mathbb R^d \rightarrow \mathbb R\), а число функций \(n\) велико. Например, в задаче обучения моделей машинного обучения, \(n\) соответствует размеру обучающей выборки, а \(f_i(x)\) — функции потерь модели на \(i\)-м объекте, где \(x\) — вектор параметров модели.

Стохастические методы хорошо подходят для данной задачи, так как позволяют избежать вычисления полного градиента на каждой итерации. В условиях реальных задач, где \(n\) может быть чрезвычайно велико, такие вычисления становятся крайне ресурсоёмкими. Одним из наиболее известных методов является стохастический градиентный спуск (\textsc{SGD}) и его модификации. На \(t\)-й итерации метод выбирает индекс \(i_t\in\{1,\ldots,n\}\) и выполняет шаг:
\[
x^{t+1} = x^t - \gamma \nabla f_{i_t}(x^t),
\]
где \(\gamma\) — величина шага метода. При этом $\nabla f_{i_t}(x^t)$ называют \textit{стохастической оценкой градиента} функции \(f\) в точке \(x^t\) по объекту \(i_t\). 

\subsection*{Методы уменьшения дисперсии}

Несмотря на простоту \textsc{SGD}, он обладает существенным недостатком: дисперсия стохастических оценок градиента сохраняется большой на протяжении всего обучения. В результате, при использовании постоянного шага, метод сходится лишь к окрестности оптимального решения, размер которой зависит от дисперсии. Для решения проблемы высокой дисперсии стохастических градиентных методов были предложены методы уменьшения дисперсии (Variance Reduction, VR), такие как \textsc{SAG}~\cite{schmidt2017minimizing}, \textsc{SAGA}~\cite{defazio2014saga}, \textsc{Finito}~\cite{defazio2014finito}, \textsc{SPIDER}~\cite{fang2018spider} и \textsc{SARAH}~\cite{nguyen2017sarah}. Эти методы основаны на идее построения более точных оценок градиента, что позволяет существенно улучшить скорость сходимости по сравнению с классическим \textsc{SGD}~\cite{robbins1951stochastic}. В частности, метод \textsc{SARAH} использует рекурсивное обновление оценок градиента и обладает как теоретическими, так и практическими преимуществами при оптимизации больших моделей. 
\begin{algorithm}[H]
\caption{\textsc{SARAH}: StochAstic Recursive grAdient algoritHm}
\label{alg:sarah-base}
\begin{algorithmic}[1]
\Require начальная точка $x^0$, шаг $\gamma$, период обновления $m$
\For{$s = 0, 1, 2, \dots$}
    \State $v^0 = \nabla f(x^0)$
    \State $x^1 = x^0 - \gamma v^0$
    \For{$t = 1$ to $m$}
        \State выбрать $i_t \sim \mathrm{Uniform}(\{1, \dots, n\})$
        \State $v^t = \nabla f_{i_t}(x^t) - \nabla f_{i_t}(x^{t-1}) + v^{t-1}$
        \State $x^{t+1} = x^t - \gamma v^t$
    \EndFor
    \State $x^0 \gets x^{m+1}$ \Comment{Перезапуск}
\EndFor
\end{algorithmic}
\end{algorithm}

\vspace{1ex}

В данной работе анализу подвергается метод \textsc{SARAH} (алгоритм~\ref{alg:sarah-base}), в котором оценка градиента обновляется рекурсивно:
\begin{gather*}
v^t = \nabla f_{i_t} (x^t) - \nabla f_{i_t} (x^{t-1}) + v^{t-1}, \\ x^{t+1} = x^t - \gamma v^t,
\end{gather*}
где $\gamma$ — величина шага оптимизации. Основная интуиция данного метода состоит в том, что мы обновляем оценку полного градиента $v^t$ (которая в идеале равна среднему градиенту по всем батчам) разницей градиентов по текущему батчу текущего и предыдущего шага. Конечно, в таком случае это даже не является оценкой полного градиента (так как градиенты по батчам в сумму входят с коэффициентом $\frac{1}{n}$), но тем не менее позволяет уменьшить дисперсию стохастических градиентов, что в свою очередь приводит к более быстрой сходимости метода.
Для достижения сходимости к оптимальному решению $x^*$ требуется периодически пересчитывать $v^t$ с использованием полного градиента. Эта процедура выполняется либо через фиксированное число итераций, либо случайным образом. Практическая версия метода, известная как \textsc{SARAH+} \citep{nguyen2017sarah}, использует правило автоматического выбора момента обновления, основываясь на величине отношения $\|v^t\|/\|v^0\|$.

\subsection*{Эвристика перемешивания}

Важным, но часто недооцениваемым аспектом стохастических методов является способ выбора индексов на каждой итерации. Это напрямую влияет как на стабильность, так и на сходимость. Вместо случайного выбора на каждой итерации, в настоящей работе применяется эвристика перемешивания. Изначально создаётся случайная перестановка индексов \(\{1,\ldots,n\}\), после чего на каждой итерации используется соответствующий элемент этой перестановки.

Наиболее известные варианты включают:
\begin{itemize}
    \item \textbf{Random Reshuffle (RR)} — данные перемешиваются перед каждой эпохой;
    \item \textbf{Shuffle Once (SO)} — перемешивание только один раз в начале обучения;
    \item \textbf{Cyclic} — данные проходят в фиксированном порядке без перемешивания.
\end{itemize}

Во всех вариантах перемешивания градиент для каждого объекта вычисляется ровно один раз за эпоху. При этом, выбор по перестановке нарушает свойство несмещённости градиентных оценок:
\[
\mathbb{E}_{\pi_s^t} \left[\nabla f_{\pi_s^t}(x_s^t)\right] \neq \nabla f(x_s^t),
\]
что приводит к более сложному анализу и нестандартным техникам доказательства.

\clearpage
\section{Обзор литературы}

\subsection*{Методы без вычисления полного градиента}

Метод \textsc{SARAH}~\cite{nguyen2017sarah} на сегодняшний день является одним из стандартных подходов к решению задачи минимизации суммы. Однако, классическая версия требует периодического вычисления полного градиента. В связи с этим был проявлен интерес к вариантам, избегающим таких вычислений.

Методы \textsc{SAG}~\cite{schmidt2017minimizing} и \textsc{SAGA}~\cite{defazio2014saga} решают эту задачу, но требуют хранения дополнительных градиентов, что ведёт к затратам памяти порядка \(\mathcal{O}(nd)\), где $d$~--- размерность вектора весов, иначе количество параметров, что в современных задачах может достигать десятки миллионов и даже больше. Ряд подходов был предложен для модификации \textsc{SARAH}, исключающей необходимость в полном градиенте.

\begin{itemize}
    \item В работе \citep{nguyen2021inexact} предложен алгоритм inexact-\textsc{SARAH}, в котором полный градиент заменяется на минибатч-оценку (то есть средний градиент по некоторому подмножеству батчей):
    \[
    \frac{1}{|S|} \sum_{i \in S} f_i(x), \quad S \subset \{1, \ldots, n\}.
    \]
    \item В других работах предлагается гибридная схема без рестартов:
    \[
    v^t = \beta_t \nabla f_{i_t}(x^t) + (1 - \beta_t)(\nabla f_{i_t}(x^t) - \nabla f_{i_t}(x^{t-1}) + v^{t-1}),
    \]
    где параметр \(\beta_t\) либо постоянен \citep{liu2020optimal}, либо стремится к нулю, как в методе \textsc{STORM} \citep{cutkosky2019momentum}. Метод \textsc{ZeroSARAH} \citep{li2021zerosarah} сочетает такую схему с \textsc{SAG/SAGA}.
\end{itemize}

\subsection*{Методы с перемешиванием}

Так как рассматриваемый метод использует эвристику перемешивания, необходимо проанализировать существующие подходы. При выборке без возвращения к уже учтенным батчам на протяжении эпохи каждый объект используется ровно один раз. Было показано, что Random Reshuffle может сходиться быстрее, чем \textsc{SGD}, на практике.

Однако теоретические оценки долгое время отставали от классических методов с независимым выбором индексов. Прорыв был достигнут в работе \citep{mishchenko2020random}, где представлены новые методы анализа. Для сильно выпуклых задач получены такие же оценки, как у \textsc{SGD} с независимым выбором. Однако в невыпуклом случае результаты остались слабее, а также требовали большого числа эпох, что не характерно для современных нейросетей. Альтернатива была предложена в \citep{koloskova2024convergence}, где анализ строится на так называемом «периоде корреляции» вместо полной эпохи.

Позднее эвристика перемешивания была применена к более общим задачам вариационных неравенств и, в частности, к методу \textsc{Extragradient}. Это позволило получить аналогичные линейные оценки. В дальнейшем внимание было сосредоточено на сочетании перемешивания с методами уменьшения дисперсии.


\begin{table}[htbp]
\centering
\small
\caption{Сравнение оценок сходимости различных алгоритмов. Красным цветом выделены оценки, у которых выигрывает алгоритм \textsc{SARAH} из данной работы.}
\label{table1}
\begin{threeparttable}
\begin{tabular}{|c|c|c|c|c|}
\hline
\rotatebox{0}{\shortstack[l]{\textbf{Алгоритм}}} & 
\rotatebox{90}{\shortstack[l]{\textbf{Без} \\ \textbf{полного} \\ \textbf{градиента?}}} &
\rotatebox{90}{\shortstack[l]{\textbf{Память}}} &
\rotatebox{90}{\shortstack[l]{\textbf{Невыпуклая} \\ \textbf{задача}}} &
\rotatebox{90}{\shortstack[l]{\textbf{Сильно} \\ \textbf{выпуклая} \\ \textbf{задача}}} \\
\hline
SAGA \cite{park2020linear} & \cmark &  \red{\(\mathcal{O}(nd)\)} & \(\backslash\) & \(\mathcal{O}\left(n\frac{L^{\red{2}}}{\mu^{\red{2}}}\log\left(\frac{1}{\varepsilon}\right)\right)\) \\ \hline
IAG \cite{gurbuzbalaban2017convergence} & \cmark &  \red{\(\mathcal{O}(nd)\)} & \(\backslash\) & \(\mathcal{O}\left(n^{\red{2}}\frac{L^{\red{2}}}{\mu^{\red{2}}}\log\left(\frac{1}{\varepsilon}\right)\right)\) \\ \hline
PIAG \cite{vanli2016stronger} & \cmark &  \red{\(\mathcal{O}(nd)\)} & \(\backslash\) & \(\mathcal{O}\left(n\frac{L}{\mu}\log\left(\frac{1}{\varepsilon}\right)\right)\) \\ \hline
DIAG \cite{mokhtari2018surpassing} & \cmark &  \red{\(\mathcal{O}(nd)\)} & \(\backslash\) & \(\mathcal{O}\left(n\frac{L}{\mu}\log\left(\frac{1}{\varepsilon}\right)\right)\) \\ \hline
Prox-DFinito \cite{huang2021improved} & \cmark & \red{\(\mathcal{O}(nd)\)} & \(\backslash\) & \(\mathcal{O}\left(n\frac{L}{\mu}\log\left(\frac{1}{\varepsilon}\right)\right)\) \\ \hline
AVRG \cite{ying2020variance} & \cmark &  \(\mathcal{O}(d)\) & \(\backslash\) & \(\mathcal{O}\left(n\frac{L^{\red{2}}}{\mu^{\red{2}}}\log\left(\frac{1}{\varepsilon}\right)\right)\) \\ \hline
SVRG \cite{sun2019general} & \xmark &  \(\mathcal{O}(d)\) & \(\backslash\) & \(\mathcal{O}\left(n^{\red{3}}\frac{L^{\red{2}}}{\mu^{\red{2}}}\log\left(\frac{1}{\varepsilon}\right)\right)\) \\ \hline
SVRG \cite{malinovsky2023random} & \xmark &  \(\mathcal{O}(d)\) & \(\mathcal{O}\left(\frac{nL}{\varepsilon^2}\right)\) & \(\mathcal{O}\left(n\frac{L^{\red{\nicefrac{3}{2}}}}{\mu^{\red{\nicefrac{3}{2}}}}\log\left(\frac{1}{\varepsilon}\right)\right)^{{\text{(1)}}}\) \\ \hline
SARAH \cite{beznosikov2023random} & \cmark & \(\mathcal{O}(d)\) & \(\backslash\) & \(\mathcal{O}\left(n^{\red{2}}\frac{L}{\mu}\log\left(\frac{1}{\varepsilon}\right)\right)\) \\ \hline
\rowcolor{yellow} SARAH (данная работа) & \cmark & \(\mathcal{O}(d)\) & \(\mathcal{O}\left(\frac{nL}{\varepsilon^2}\right)\) & \(\mathcal{O}\left(n\frac{L}{\mu}\log\left(\frac{1}{\varepsilon}\right)\right)\) \\ \hline
\end{tabular}
\begin{tablenotes}
\vspace{1em}
\begin{minipage}{0.9\textwidth}
    \item[] \textit{Столбцы:} «Без полного градиента?» — указывает, требует ли метод вычисления полного градиента. «Память» — дополнительные затраты по памяти.
    \item[] \textit{Обозначения:} $\mu$ — коэффициент сильной выпуклости, $L$ — константа гладкости, $n$ — размер выборки, $d$ — размерность задачи, $\varepsilon$ — требуемая точность.
    \item[] {(1)} В данной работе также получены улучшенные оценки в случае больших данных: $n \gg \mathcal{O}\left(\frac{L}{\mu}\right)$, однако они выходят за рамки настоящего анализа.
\end{minipage}
\end{tablenotes}    
\end{threeparttable}
\end{table}
