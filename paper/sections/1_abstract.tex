\begin{center}
    \Large{\textbf{Аннотация}}
\end{center}

    В современном мире машинное обучение невозможно представить без использования больших обучающих выборок и моделей. Это обусловило широкое применение стохастических методов обучения, таких как \textsc{SGD}. Несмотря на простоту, \textsc{SGD} обладает слабыми теоретическими гарантиями сходимости, связанными с неубывающей дисперсией. Данная проблема может быть частично устранена с помощью модификаций, таких как \textsc{SARAH}. Однако эти методы требуют периодического вычисления полного градиента, что может быть затратным по времени. В данной работе были рассмотрены варианты алгоритмов с уменьшением дисперсии, не предполагающие необходимость вычисления полного градиента. Для повышения эффективности по памяти и исключения этих вычислений были использованы два ключевых подхода: эвристика перемешивания и идея, лежащая в основе методов \textsc{SAG/SAGA}. В результате были улучшены существующие оценки для алгоритмов с уменьшением дисперсии без вычисления полного градиента. В случае невыпуклой целевой функции полученная оценка совпадает с классическими методами на основе перемешивания, а для сильно выпуклой задачи достигается улучшение. Проведён всесторонний теоретический анализ, а также представлены масштабные экспериментальные результаты, подтверждающие эффективность и практическую применимость предложенных методов в задачах обучения на больших данных.