@article{mishchenko2020random,
  title={Random reshuffling: Simple analysis with vast improvements},
  author={Mishchenko, Konstantin and Khaled, Ahmed and Richt{\'a}rik, Peter},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={17309--17320},
  year={2020}
}

@article{koloskova2023shuffle,
  title={Shuffle SGD is Always Better than SGD: Improved Analysis of SGD with Arbitrary Data Orders},
  author={Koloskova, Anastasia and Doikov, Nikita and Stich, Sebastian U and Jaggi, Martin},
  journal={arXiv preprint arXiv:2305.19259},
  year={2023}
}

@inproceedings{gorbunov2020unified,
  title={A unified theory of SGD: Variance reduction, sampling, quantization and coordinate descent},
  author={Gorbunov, Eduard and Hanzely, Filip and Richt{\'a}rik, Peter},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={680--690},
  year={2020},
  organization={PMLR}
}

@article{li2020unified,
  title={A unified analysis of stochastic gradient methods for nonconvex federated optimization},
  author={Li, Zhize and Richt{\'a}rik, Peter},
  journal={arXiv preprint arXiv:2006.07013},
  year={2020}
}

@inproceedings{malinovsky2023random,
  title={Random reshuffling with variance reduction: New analysis and better rates},
  author={Malinovsky, Grigory and Sailanbayev, Alibek and Richt{\'a}rik, Peter},
  booktitle={Uncertainty in Artificial Intelligence},
  pages={1347--1357},
  year={2023},
  organization={PMLR}
}

@inproceedings{mishchenko2022proximal,
  title={Proximal and federated random reshuffling},
  author={Mishchenko, Konstantin and Khaled, Ahmed and Richt{\'a}rik, Peter},
  booktitle={International Conference on Machine Learning},
  pages={15718--15749},
  year={2022},
  organization={PMLR}
}

@article{vanli2016stronger,
  title={A stronger convergence result on the proximal incremental aggregated gradient method},
  author={Vanli, Nuri Denizcan and Gurbuzbalaban, Mert and Ozdaglar, Asu},
  journal={arXiv preprint arXiv:1611.08022},
  year={2016}
}

@article{ying2020variance,
  title={Variance-reduced stochastic learning under random reshuffling},
  author={Ying, Bicheng and Yuan, Kun and Sayed, Ali H},
  journal={IEEE Transactions on Signal Processing},
  volume={68},
  pages={1390--1408},
  year={2020},
  publisher={IEEE}
}

@article{sun2019general,
  title={General proximal incremental aggregated gradient algorithms: Better and novel results under general scheme},
  author={Sun, Tao and Sun, Yuejiao and Li, Dongsheng and Liao, Qing},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@article{mokhtari2018surpassing,
  title={Surpassing gradient descent provably: A cyclic incremental method with linear convergence rate},
  author={Mokhtari, Aryan and Gurbuzbalaban, Mert and Ribeiro, Alejandro},
  journal={SIAM Journal on Optimization},
  volume={28},
  number={2},
  pages={1420--1447},
  year={2018},
  publisher={SIAM}
}

@article{chow2017cyclic,
  title={Cyclic coordinate-update algorithms for fixed-point problems: Analysis and applications},
  author={Chow, Yat Tin and Wu, Tianyu and Yin, Wotao},
  journal={SIAM Journal on Scientific Computing},
  volume={39},
  number={4},
  pages={A1280--A1300},
  year={2017},
  publisher={SIAM}
}

@article{huang2021improved,
  title={An improved analysis and rates for variance reduction under without-replacement sampling orders},
  author={Huang, Xinmeng and Yuan, Kun and Mao, Xianghui and Yin, Wotao},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={3232--3243},
  year={2021}
}

@article{zhang2020tight,
  title={Tight Lower Complexity Bounds for Strongly Convex Finite-Sum Optimization},
  author={Zhang, Min and Shu, Yao and He, Kun},
  journal={arXiv preprint arXiv:2010.08766},
  year={2020}
}

@inproceedings{alacaoglu2022stochastic,
  title={Stochastic variance reduction for variational inequality methods},
  author={Alacaoglu, Ahmet and Malitsky, Yura},
  booktitle={Conference on Learning Theory},
  pages={778--816},
  year={2022},
  organization={PMLR}
}

@article{stich2019unified,
  title={Unified optimal analysis of the (stochastic) gradient method},
  author={Stich, Sebastian U},
  journal={arXiv preprint arXiv:1907.04232},
  year={2019}
}

@article{beznosikov2023random,
  title={Random-reshuffled SARAH does not need full gradient computations},
  author={Beznosikov, Aleksandr and Tak{\'a}{\v{c}}, Martin},
  journal={Optimization Letters},
  pages={1--23},
  year={2023},
  publisher={Springer}
}

@inproceedings{bottou2009curiously,
  title={Curiously fast convergence of some stochastic gradient descent algorithms},
  author={Bottou, L{\'e}on},
  booktitle={Proceedings of the symposium on learning and data science, Paris},
  volume={8},
  pages={2624--2633},
  year={2009},
  organization={Citeseer}
}

@article{recht2013parallel,
  title={Parallel stochastic gradient algorithms for large-scale matrix completion},
  author={Recht, Benjamin and R{\'e}, Christopher},
  journal={Mathematical Programming Computation},
  volume={5},
  number={2},
  pages={201--226},
  year={2013},
  publisher={Springer}
}

@article{rakhlin2012making,
  title={Making Gradient Descent Optimal for Strongly Convex Stochastic Optimization},
  author={Rakhlin, Alexander and Shamir, Ohad and Sridharan, Karthik},
  year={2012}
}

@inproceedings{drori2020complexity,
  title={The complexity of finding stationary points with stochastic gradient descent},
  author={Drori, Yoel and Shamir, Ohad},
  booktitle={International Conference on Machine Learning},
  pages={2658--2667},
  year={2020},
  organization={PMLR}
}

@inproceedings{nguyen2019tight,
  title={Tight dimension independent lower bound on the expected convergence rate for diminishing step sizes in sgd},
  author={Nguyen, PH and Nguyen, LM and van Dijk, M},
  booktitle={33rd Annual Conference on Neural Information Processing Systems, NeurIPS 2019},
  year={2019},
  organization={Neural information processing systems foundation}
}

@article{gurbuzbalaban2017convergence,
  title={On the convergence rate of incremental aggregated gradient algorithms},
  author={Gurbuzbalaban, Mert and Ozdaglar, Asuman and Parrilo, Pablo A},
  journal={SIAM Journal on Optimization},
  volume={27},
  number={2},
  pages={1035--1048},
  year={2017},
  publisher={SIAM}
}

@article{park2020linear,
  title={Linear convergence of cyclic SAGA},
  author={Park, Youngsuk and Ryu, Ernest K},
  journal={Optimization Letters},
  volume={14},
  number={6},
  pages={1583--1598},
  year={2020},
  publisher={Springer}
}

@book{nesterov2018lectures,
  title={Lectures on convex optimization},
  author={Nesterov, Yurii and others},
  volume={137},
  year={2018},
  publisher={Springer}
}

@article{gower2020variance,
  title={Variance-reduced methods for machine learning},
  author={Gower, Robert M and Schmidt, Mark and Bach, Francis and Richt{\'a}rik, Peter},
  journal={Proceedings of the IEEE},
  volume={108},
  number={11},
  pages={1968--1983},
  year={2020},
  publisher={IEEE}
}

@article{johnson2013accelerating,
  title={Accelerating stochastic gradient descent using predictive variance reduction},
  author={Johnson, Rie and Zhang, Tong},
  journal={Advances in neural information processing systems},
  volume={26},
  year={2013}
}

@article{roux2012stochastic,
  title={A stochastic gradient method with an exponential convergence \_rate for finite training sets},
  author={Roux, Nicolas and Schmidt, Mark and Bach, Francis},
  journal={Advances in neural information processing systems},
  volume={25},
  year={2012}
}

@article{defazio2014saga,
  title={SAGA: A fast incremental gradient method with support for non-strongly convex composite objectives},
  author={Defazio, Aaron and Bach, Francis and Lacoste-Julien, Simon},
  journal={Advances in neural information processing systems},
  volume={27},
  year={2014}
}

@inproceedings{defazio2014finito,
  title={Finito: A faster, permutable incremental gradient method for big data problems},
  author={Defazio, Aaron and Domke, Justin and others},
  booktitle={International Conference on Machine Learning},
  pages={1125--1133},
  year={2014},
  organization={PMLR}
}

@article{nguyen2021unified,
  title={A unified convergence analysis for shuffling-type gradient methods},
  author={Nguyen, Lam M and Tran-Dinh, Quoc and Phan, Dzung T and Nguyen, Phuong Ha and Van Dijk, Marten},
  journal={Journal of Machine Learning Research},
  volume={22},
  number={207},
  pages={1--44},
  year={2021}
}

@article{robbins1951stochastic,
  title={A stochastic approximation method},
  author={Robbins, Herbert and Monro, Sutton},
  journal={The annals of mathematical statistics},
  pages={400--407},
  year={1951},
  publisher={JSTOR}
}

@inproceedings{mcmahan2017communication,
  title={Communication-efficient learning of deep networks from decentralized data},
  author={McMahan, Brendan and Moore, Eider and Ramage, Daniel and Hampson, Seth and y Arcas, Blaise Aguera},
  booktitle={Artificial intelligence and statistics},
  pages={1273--1282},
  year={2017},
  organization={PMLR}
}

@book{lan2020first,
  title={First-order and stochastic optimization methods for machine learning},
  author={Lan, Guanghui},
  volume={1},
  year={2020},
  publisher={Springer}
}

@misc{koloskova2024convergence,
      title={On Convergence of Incremental Gradient for Non-Convex Smooth Functions}, 
      author={Anastasia Koloskova and Nikita Doikov and Sebastian U. Stich and Martin Jaggi},
      year={2024},
      eprint={2305.19259},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{moulines2011non,
  title={Non-asymptotic analysis of stochastic approximation algorithms for machine learning},
  author={Moulines, Eric and Bach, Francis},
  journal={Advances in neural information processing systems},
  volume={24},
  year={2011}
}

@inproceedings{li2021page,
  title={PAGE: A simple and optimal probabilistic gradient estimator for nonconvex optimization},
  author={Li, Zhize and Bao, Hongyan and Zhang, Xiangliang and Richt{\'a}rik, Peter},
  booktitle={International conference on machine learning},
  pages={6286--6295},
  year={2021},
  organization={PMLR}
}

@inproceedings{nguyen2017sarah,
  title={SARAH: A novel method for machine learning problems using stochastic recursive gradient},
  author={Nguyen, Lam M and Liu, Jie and Scheinberg, Katya and Tak{\'a}{\v{c}}, Martin},
  booktitle={International conference on machine learning},
  pages={2613--2621},
  year={2017},
  organization={PMLR}
}

@article{hu2019efficient,
  title={Efficient smooth non-convex stochastic compositional optimization via stochastic recursive gradient descent},
  author={Hu, Wenqing and Li, Chris Junchi and Lian, Xiangru and Liu, Ji and Yuan, Huizhuo},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@article{fang2018spider,
  title={Spider: Near-optimal non-convex optimization via stochastic path-integrated differential estimator},
  author={Fang, Cong and Li, Chris Junchi and Lin, Zhouchen and Zhang, Tong},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@inproceedings{safran2020good,
  title={How good is SGD with random shuffling?},
  author={Safran, Itay and Shamir, Ohad},
  booktitle={Conference on Learning Theory},
  pages={3250--3284},
  year={2020},
  organization={PMLR}
}

@article{ghadimi2013stochastic,
  title={Stochastic first-and zeroth-order methods for nonconvex stochastic programming},
  author={Ghadimi, Saeed and Lan, Guanghui},
  journal={SIAM journal on optimization},
  volume={23},
  number={4},
  pages={2341--2368},
  year={2013},
  publisher={SIAM}
}

@article{chang2011libsvm,
  title={LIBSVM: a library for support vector machines},
  author={Chang, Chih-Chung and Lin, Chih-Jen},
  journal={ACM transactions on intelligent systems and technology (TIST)},
  volume={2},
  number={3},
  pages={1--27},
  year={2011},
  publisher={Acm New York, NY, USA}
}

@article{chambolle2011first,
  title={A first-order primal-dual algorithm for convex problems with applications to imaging},
  author={Chambolle, Antonin and Pock, Thomas},
  journal={Journal of mathematical imaging and vision},
  volume={40},
  pages={120--145},
  year={2011},
  publisher={Springer}
}

@inproceedings{allen2016variance,
  title={Variance reduction for faster non-convex optimization},
  author={Allen-Zhu, Zeyuan and Hazan, Elad},
  booktitle={International conference on machine learning},
  pages={699--707},
  year={2016},
  organization={PMLR}
}

@inproceedings{kovalev2020don,
  title={Donâ€™t jump through hoops and remove those loops: SVRG and Katyusha are better without the outer loop},
  author={Kovalev, Dmitry and Horv{\'a}th, Samuel and Richt{\'a}rik, Peter},
  booktitle={Algorithmic Learning Theory},
  pages={451--467},
  year={2020},
  organization={PMLR}
}

@article{allen2018katyusha,
  title={Katyusha: The first direct acceleration of stochastic gradient methods},
  author={Allen-Zhu, Zeyuan},
  journal={Journal of Machine Learning Research},
  volume={18},
  number={221},
  pages={1--51},
  year={2018}
}

@inproceedings{li2020convergence,
  title={On the convergence of SARAH and beyond},
  author={Li, Bingcong and Ma, Meng and Giannakis, Georgios B},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={223--233},
  year={2020},
  organization={PMLR}
}

@article{li2021zerosarah,
  title={ZeroSARAH: Efficient nonconvex finite-sum optimization with zero full gradient computation},
  author={Li, Zhize and Hanzely, Slavom{\'\i}r and Richt{\'a}rik, Peter},
  journal={arXiv preprint arXiv:2103.01447},
  year={2021}
}

@article{nguyen2021inexact,
  title={Inexact SARAH algorithm for stochastic optimization},
  author={Nguyen, Lam M and Scheinberg, Katya and Tak{\'a}{\v{c}}, Martin},
  journal={Optimization Methods and Software},
  volume={36},
  number={1},
  pages={237--258},
  year={2021},
  publisher={Taylor \& Francis}
}

@article{liu2020optimal,
  title={An optimal hybrid variance-reduced algorithm for stochastic composite nonconvex optimization},
  author={Liu, Deyi and Nguyen, Lam M and Tran-Dinh, Quoc},
  journal={arXiv preprint arXiv:2008.09055},
  year={2020}
}

@article{cutkosky2019momentum,
  title={Momentum-based variance reduction in non-convex sgd},
  author={Cutkosky, Ashok and Orabona, Francesco},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{schmidt2017minimizing,
  title={Minimizing finite sums with the stochastic average gradient},
  author={Schmidt, Mark and Le Roux, Nicolas and Bach, Francis},
  journal={Mathematical Programming},
  volume={162},
  pages={83--112},
  year={2017},
  publisher={Springer}
}

@article{arjevani2023lower,
  title={Lower bounds for non-convex stochastic optimization},
  author={Arjevani, Yossi and Carmon, Yair and Duchi, John C and Foster, Dylan J and Srebro, Nathan and Woodworth, Blake},
  journal={Mathematical Programming},
  volume={199},
  number={1},
  pages={165--214},
  year={2023},
  publisher={Springer}
}

@article{metelev2024decentralized,
  title={Decentralized finite-sum optimization over time-varying networks},
  author={Metelev, Dmitry and Chezhegov, Savelii and Rogozin, Alexander and Beznosikov, Aleksandr and Sholokhov, Alexander and Gasnikov, Alexander and Kovalev, Dmitry},
  journal={arXiv preprint arXiv:2402.02490},
  year={2024}
}
@inproceedings{zhou2019lower,
  title={Lower bounds for smooth nonconvex finite-sum optimization},
  author={Zhou, Dongruo and Gu, Quanquan},
  booktitle={International Conference on Machine Learning},
  pages={7574--7583},
  year={2019},
  organization={PMLR}
}

@article{krizhevsky2009learning,
  title={Learning multiple layers of features from tiny images},
  author={Krizhevsky, Alex and Hinton, Geoffrey and others},
  year={2009},
  publisher={Toronto, ON, Canada}
}

@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}

@article{paszke2019pytorch,
  title={Pytorch: An imperative style, high-performance deep learning library},
  author={Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@inproceedings{medyakov2024shuffling,
  title={Shuffling Heuristic in Variational Inequalities: Establishing New Convergence Guarantees},
  author={Medyakov, Daniil and Molodtsov, Gleb and Grigoriy, Evseev and Petrov, Egor and Beznosikov, Aleksandr},
  booktitle={International Conference on Computational Optimization},
  year={2024}
}

@article{beznosikov2023smooth,
  title={Smooth monotone stochastic variational inequalities and saddle point problems: A survey},
  author={Beznosikov, Aleksandr and Polyak, Boris and Gorbunov, Eduard and Kovalev, Dmitry and Gasnikov, Alexander},
  journal={European Mathematical Society Magazine},
  number={127},
  pages={15--28},
  year={2023}
}

@article{juditsky2011solving,
  title={Solving variational inequalities with stochastic mirror-prox algorithm},
  author={Juditsky, Anatoli and Nemirovski, Arkadi and Tauvel, Claire},
  journal={Stochastic Systems},
  volume={1},
  number={1},
  pages={17--58},
  year={2011},
  publisher={INFORMS}
}

@article{ghadimi2016mini,
  title={Mini-batch stochastic approximation methods for nonconvex stochastic composite optimization},
  author={Ghadimi, Saeed and Lan, Guanghui and Zhang, Hongchao},
  journal={Mathematical Programming},
  volume={155},
  number={1},
  pages={267--305},
  year={2016},
  publisher={Springer}
}

@inproceedings{swin,
  title={Swin transformer: Hierarchical vision transformer using shifted windows},
  author={Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={10012--10022},
  year={2021}
}

@inproceedings{imagenet,
  title={Imagenet: A large-scale hierarchical image database},
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle={2009 IEEE conference on computer vision and pattern recognition},
  pages={248--255},
  year={2009},
  organization={Ieee}
}

@article{tiny_imagenet,
  title={Tiny imagenet visual recognition challenge},
  author={Le, Yann and Yang, Xuan},
  journal={CS 231N},
  volume={7},
  number={7},
  pages={3},
  year={2015}
}